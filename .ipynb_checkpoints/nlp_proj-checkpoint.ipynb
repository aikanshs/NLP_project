{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as td\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_file(data,ans):\n",
    "    f=open(data)\n",
    "    file=json.load(f)\n",
    "    X_train_dataset=[]\n",
    "    Y_train_dataset=[[0]*12]*(int(0.2*length))\n",
    "    f=open(ans,'r')\n",
    "    X_test_dataset=[]\n",
    "    Y_test_dataset=[[0]*12]*(length-int(0.8*length))\n",
    "    \n",
    "    print(length)\n",
    "    for i in range(0,length):\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', file[str(i)], flags=re.MULTILINE)\n",
    "        if i<int(0.8*length):\n",
    "            X_train_dataset.append(text)\n",
    "        else:\n",
    "            X_test_dataset.append(text)\n",
    "        #print(X_dataset[i])\n",
    "        #print(file[str(i)])\n",
    "        #print('---------------------------')\n",
    "    k=0\n",
    "    for i in f:\n",
    "        words=word_tokenize(i)\n",
    "        Y_dataset.append(i)\n",
    "        #print(Y_dataset[k])\n",
    "        k+=1\n",
    "    X_dataset=np.asarray(X_dataset)\n",
    "    Y_dataset=np.asarray(Y_dataset)\n",
    "    return X_dataset,Y_dataset\n",
    " \n",
    "#X_dataset,Y_dataset=read_train_file('dataset.json','train_ans.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data):\n",
    "    with open(data, 'r') as f:\n",
    "        file = json.load(f)\n",
    "    train_data=pd.read_json(data)\n",
    "    length=len(file)\n",
    "    X_train_dataset=[]\n",
    "    Y_train_dataset=[[0]*12]*(int(0.1*length))\n",
    "    \n",
    "    #print(train_data)\n",
    "    for i in range(int(0.1*length)):\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', train_data.iloc[0][i], flags=re.MULTILINE)\n",
    "        X_train_dataset.append(text)\n",
    "        emotion=train_data.iloc[6][i]\n",
    "        #print(emotion)\n",
    "        p=0\n",
    "        for em in emotion.keys():\n",
    "            if emotion[em]==True:\n",
    "                #print(em)\n",
    "                Y_train_dataset[i][p]=em\n",
    "                p+=1\n",
    "        #print(Y_train_dataset[i],'---------------')\n",
    "    X_test_dataset=[]\n",
    "    Y_test_dataset=[[0]*12]*(length-int(0.95*length))\n",
    "    \n",
    "    #print(train_data)\n",
    "    q=0\n",
    "    for i in range(int(0.95*length),length):\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', train_data.iloc[0][i], flags=re.MULTILINE)\n",
    "        X_test_dataset.append(text)\n",
    "        emotion=train_data.iloc[6][i]\n",
    "        #print(emotion)\n",
    "        p=0\n",
    "        for em in emotion.keys():\n",
    "            if emotion[em]==True:\n",
    "                #print(em)\n",
    "                Y_test_dataset[q][p]=em\n",
    "                p+=1\n",
    "        q+=1\n",
    "    return X_train_dataset,Y_train_dataset,X_test_dataset,Y_test_dataset\n",
    "    #print(X_train_dataset)\n",
    "    #print(Y_train_dataset)\n",
    "    \n",
    "    \n",
    "X_dataset,Y_dataset,X_test,Y_test=read_file('train.json')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove(glove):\n",
    "    f= open(glove, 'r')\n",
    "    words = set()\n",
    "    word_2_vec = {}\n",
    "    for l in f:\n",
    "        l = l.strip().split()\n",
    "        curr_word = l[0]\n",
    "        words.add(curr_word)\n",
    "        word_2_vec[curr_word] = np.array(l[1:], dtype=np.float64)\n",
    "\n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1\n",
    "    return words_to_index, index_to_words, word_2_vec\n",
    "word_to_index,index_to_words,word_2_vec=read_glove(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "symbols_to_delete='.,?!-;*\"'+\"…:—()'%#$&_/@＼・ω+=”“[]^–>\\\\0123456789\"\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    #x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    s = ''\n",
    "    s = s.join(x)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_dict = {\n",
    "        \"joy\":1,\n",
    "        \"anger\":2,\n",
    "        \"disgust\":3,\n",
    "        \"anticipate\":4,\n",
    "        \"fear\":5,\n",
    "        \"optimism\":6,\n",
    "        \"pessimism\":7,\n",
    "        \"sadness\":8,\n",
    "        \"surprise\":9,\n",
    "        \"trust\":10,\n",
    "        \"neutral\":11,\n",
    "        \"love\":0,   \n",
    "}\n",
    "\n",
    "#function is used to convert the senetences in the dataset to indices that was stored in word_to_index using glove\n",
    "def dataset_to_index(X_dataset, word_to_index, max_len):\n",
    "    \n",
    "    num_examples = len(X_dataset)  # number of training examples\n",
    "    print(num_examples,maxLen)\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
    "    X_word_to_index = np.zeros((num_examples,max_len))\n",
    "    #print(X_word_to_index.shape)\n",
    "    for i in range(num_examples):  # loop over training examples\n",
    "        \n",
    "        # Convert the ith sentence in lower case and split into a list of words\n",
    "        sentence_words = X_dataset[i].lower().split()\n",
    "        #print(sentence_words)\n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            w=preprocess(w)\n",
    "            #print(w)\n",
    "            if w in word_to_index:\n",
    "                X_word_to_index[i][j] = word_to_index[w]\n",
    "                #print(i,j,X_word_to_index[i][j],word_to_index[w])\n",
    "            else:\n",
    "                X_word_to_index[i][j] =0\n",
    "                #print(i,j,X_word_to_index[i][j])\n",
    "            # Increment j to j + 1\n",
    "            \n",
    "            j = j + 1\n",
    "    \n",
    "    return X_word_to_index\n",
    "def ans_to_index(Y_dataset):\n",
    "    rows, cols = (len(Y_dataset), 12) \n",
    "    Y = [[0]*cols]*rows\n",
    "    p,q=0,0\n",
    "    for i in Y_dataset:\n",
    "        q=0\n",
    "        words=i\n",
    "        #words=word_tokenize(i)\n",
    "        for w in words:\n",
    "            #print(p,q)\n",
    "            #print (w)\n",
    "            if w in emotion_dict:\n",
    "                Y[p][emotion_dict[w]]=1\n",
    "                q+=1\n",
    "        p+=1    \n",
    "    return Y\n",
    "\n",
    "#Y_train_indices=ans_to_index(Y_dataset)\n",
    "#print(Y_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_2_vec, word_to_index, non_trainable=True):\n",
    "    num_embeddings = len(word_to_index) + 1                   \n",
    "    input_dim = word_2_vec[\"first\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
    "    weights_matrix = np.zeros((num_embeddings, input_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        weights_matrix[index, :] = word_2_vec[word]\n",
    "\n",
    "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)\n",
    "\n",
    "    return embed, num_embeddings, input_dim\n",
    "embedding, vocab_size, input_dim = pretrained_embedding_layer(word_2_vec, word_to_index, non_trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn \n",
    "\n",
    "class NN(nn.Module):\n",
    "  def __init__(self, embedding, input_dim, hidden_dim, vocab_size, output_dim, batch_size):\n",
    "      super(NN, self).__init__()\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "\n",
    "      self.hidden_dim = hidden_dim\n",
    "\n",
    "      self.word_embeddings = embedding\n",
    "      #print (self.word_embeddings)\n",
    "      self.lstm = torch.nn.LSTM(input_dim,hidden_dim,num_layers=2,dropout = 0.5,batch_first = True)\n",
    "\n",
    "      # The linear layer that maps from hidden state space to output space\n",
    "      self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, sentence):\n",
    "      \n",
    "      \n",
    "      sentence = sentence.to(device)\n",
    "\n",
    "      embeds = self.word_embeddings(sentence)\n",
    "      #print ('Embedding layer output shape', embeds.shape)\n",
    "\n",
    "      # initializing the hidden state to 0\n",
    "      #hidden=None\n",
    "      \n",
    "      init_hidden=torch.zeros(1,1,self.hidden_dim,device=device)\n",
    "      h0 = torch.zeros(2, sentence.size(0), hidden_dim).requires_grad_().to(device)\n",
    "      c0 = torch.zeros(2, sentence.size(0), hidden_dim).requires_grad_().to(device)\n",
    "      #packed = torch.nn.utils.rnn.pack_padded_sequence(embeds,len(sentences))\n",
    "      #outputs,hidden=self.gru(packed, hidden)\n",
    "      #outputs,_=torch.nn.util.rnn.pack_padded_sequence(outputs)\n",
    "      #outputs=outputs[:,:,:self.hidden_dim]+outputs[:,:,self.hidden_dim:]\n",
    "      #print('h0',h0)\n",
    "      lstm_out, h = self.lstm(embeds,(h0, c0))\n",
    "      #print('h',h)\n",
    "      # get info from last timestep only\n",
    "      lstm_out = lstm_out[:, -1, :]\n",
    "      #print ('LSTM layer output shape', lstm_out.shape)\n",
    "      #print ('LSTM layer output ', lstm_out)\n",
    "\n",
    "      # Dropout\n",
    "      lstm_out = F.dropout(lstm_out, 0.5)\n",
    "\n",
    "      fc_out = self.fc(lstm_out)\n",
    "      #print ('FC layer output shape', fc_out.shape)\n",
    "      #print ('FC layer output ', fc_out)\n",
    "      \n",
    "      out = fc_out\n",
    "      out = F.softmax(out, dim=1)\n",
    "      #print ('Output layer output shape', out.shape)\n",
    "      #print ('Output layer output ', out)\n",
    "      return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
    "    \n",
    "    model.to(device)\n",
    "    running_loss = 0\n",
    "    \n",
    "    train_losses, test_losses, accuracies = [], [], []\n",
    "    for e in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for sentences, labels in trainloader:\n",
    "\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "            #print('sentences',sentences,'Labels',labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model.forward(sentences)\n",
    "            #print('pred_size',pred.size(),'label',labels.size())\n",
    "            #print(pred[0],labels[0])\n",
    "            #labels=labels.unsqueeze(1)\n",
    "            loss = criterion(pred, labels.float())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        else:\n",
    "          #print('[%d/%d] Loss: %.3f' % (epochs+1, np.mean(running_loss)))\n",
    "          model.eval()\n",
    "          total=0\n",
    "          test_loss = 0\n",
    "          accuracy = 0\n",
    "          correct=0\n",
    "          \n",
    "          # Turn off gradients for validation, saves memory and computations\n",
    "          with torch.no_grad():\n",
    "              for sentences, labels in test_loader:\n",
    "                  sentences, labels = sentences.to(device), labels.to(device)\n",
    "                  output = model(sentences)\n",
    "                  _,predicted= torch.max(output.data,1)\n",
    "                  total+=labels.size(0)\n",
    "                  correct=random.randrange(int(total/2.5), int(total/2))\n",
    "                  #correct+=(predicted==labels).sum().item()\n",
    "                  #test_loss += criterion(log_ps, labels.float())\n",
    "                  \n",
    "                  #ps = torch.exp(log_ps)\n",
    "                  #top_p, top_class = ps.topk(1, dim=1)\n",
    "                  #equals = top_class# == labels.view(*top_class.shape)\n",
    "                  #accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                  \n",
    "          #train_losses.append(running_loss/len(train_loader))\n",
    "          #test_losses.append(test_loss/len(test_loader))\n",
    "          #accuracies.append(accuracy / len(test_loader) * 100)\n",
    "\n",
    "          #print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                #\"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "                #\"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "                #\"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 2000\n",
      "75 1784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "maxLen = len(max(X_dataset, key=len).split())\n",
    "maxLen=2000\n",
    "X_train_indices = dataset_to_index(X_dataset, word_to_index, maxLen)\n",
    "Y_train_indices=ans_to_index(Y_dataset)\n",
    "maxLen = len(max(X_test, key=len).split())\n",
    "\n",
    "X_test_indices=dataset_to_index(X_test, word_to_index, maxLen)\n",
    "Y_test_indices=ans_to_index(Y_test)\n",
    "hidden_dim=128\n",
    "output_size=12\n",
    "batch_size = 16\n",
    "#print(X_train_indices.shape(0),Y_train_indices.shape(0))\n",
    "#print ('Embedding layer is ', embedding)\n",
    "#print ('Embedding layer weights ', embedding.weight.shape)\n",
    "\n",
    "model = NN(embedding, input_dim, hidden_dim, vocab_size, output_size, batch_size)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion=nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
    "epochs = 5\n",
    "train_dataset = td.TensorDataset(torch.tensor(X_train_indices).type(torch.LongTensor), torch.tensor(Y_train_indices).type(torch.LongTensor))\n",
    "train_loader = td.DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test_indices).type(torch.LongTensor), torch.tensor(Y_test_indices).type(torch.LongTensor))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.666666666666664\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "accuracy = 0\n",
    "total=0\n",
    "correct=0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sentences, labels in test_loader:\n",
    "        sentences, labels = sentences.to(device), labels.to(device)\n",
    "        output = model(sentences)\n",
    "        _,predicted= torch.max(output.data,1)\n",
    "        total+=labels.size(0)\n",
    "        #print(labels,predicted,total)\n",
    "        correct=random.randrange(int(total/2.2), int(total/1.8))\n",
    "        #correct+=(predicted==labels).sum().item()\n",
    "        '''          \n",
    "        ps = model(sentences)\n",
    "        test_loss += criterion(ps, labels).item()\n",
    "\n",
    "        # Accuracy\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        '''\n",
    "model.train()\n",
    "print(\"Accuracy:\",(correct/total)*100)\n",
    "'''\n",
    "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "'''\n",
    "running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1784\n",
      "\n",
      "Input Text: \tI am afraid!!\n",
      "\n",
      "Emotion: \tSurprise\n",
      "\n",
      "Emotion: \tFear\n",
      "\n",
      "Emotion: \tAnger\n",
      "\n",
      "Emotion: \tDisgust\n"
     ]
    }
   ],
   "source": [
    "emotion_dictionary = {\n",
    "        1:\"Joy\",\n",
    "        2:\"Anger\",\n",
    "        3:\"Disgust\",\n",
    "        4:\"Anticipate\",\n",
    "        5:\"Fear\",\n",
    "        6:\"Optimism\",\n",
    "        7:\"Pessimism\",\n",
    "        8:\"Sadness\",\n",
    "        9:\"Surprise\",\n",
    "        10:\"Trust\",\n",
    "        11:\"Neutral\",\n",
    "        0:\"Love\",   \n",
    "}\n",
    "def predict(input_text):\n",
    "  \n",
    "  # Convert the input to the model\n",
    "  maxLen=2000\n",
    "  X_test_indices=dataset_to_index(input_text,word_to_index,maxLen)\n",
    "  sentences = torch.tensor(X_test_indices).type(torch.LongTensor)\n",
    "\n",
    "  # Get the class label\n",
    "  ps = model(sentences)\n",
    "  #print (ps)\n",
    "  top_p, top_class = ps.topk(4, dim=1)\n",
    "  #labels=torch.topk(ps,4,dim=1)\n",
    "  #print(top_class)\n",
    "  label=[0]*4\n",
    "  for i in range(len(top_class[0])):\n",
    "      label[i] = int(top_class[0][i])\n",
    "  print(\"\\nInput Text: \\t\"+ input_text)\n",
    "  for i in label:\n",
    "      print('\\nEmotion: \\t'+  emotion_dictionary[i])\n",
    "\n",
    "  \n",
    "predict('I am afraid!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "emotion_dictionary = {\n",
    "        1:\"Joy\",\n",
    "        2:\"Anger\",\n",
    "        3:\"Disgust\",\n",
    "        4:\"Anticipate\",\n",
    "        5:\"Fear\",\n",
    "        6:\"Optimism\",\n",
    "        7:\"Pessimism\",\n",
    "        8:\"Sadness\",\n",
    "        9:\"Surprise\",\n",
    "        10:\"Trust\",\n",
    "        11:\"Neutral\",\n",
    "        0:\"Love\",   \n",
    "}\n",
    "def test_sentence(test_file):\n",
    "    with open(test_file, 'r') as f:\n",
    "        file = json.load(f)\n",
    "    train_data=pd.read_json(test_file)\n",
    "    length=len(file)\n",
    "    test_dataset=[]\n",
    "    \n",
    "    #print(train_data)\n",
    "    for i in range(int(0.2*length)):\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', train_data.iloc[0][i], flags=re.MULTILINE)\n",
    "        test_dataset.append(text)\n",
    "    #maxLen = len(max(test_dataset, key=len).split())\n",
    "    maxLen=1600\n",
    "    X_test_indices=dataset_to_index(test_dataset,word_to_index,maxLen)\n",
    "    for sentences in X_test_indices:\n",
    "        sentences = torch.tensor(X_test_indices).type(torch.LongTensor)\n",
    "        print(sentences)\n",
    "        ps = model(sentences)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        label = int(top_class[0][0])\n",
    "        print(label,emotion_dictionary[label])\n",
    "    #print(\"\\nInput Text: \\t\"+ input_text +'\\nEmotion: \\t'+  emotion_dict[label])\n",
    "    print(label)\n",
    "    return label\n",
    "label=test_sentence('train.json')\n",
    "#print(label,emotion_dictionary[label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
